---
title: "Using Neural Nets to Create Beer Recipies"
author: "Ryan Volkert and Paul Chong"
date: "5/6/2021"
header-includes:
    - \usepackage{setspace}\doublespacing
output:
  pdf_document: default
  word_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(knitr)
library(tidyverse)
library(ggplot2)
library(readr)
library(reticulate)
```

```{r}
use_python('C:/Users/rvolk/anaconda3/python.exe')
```
\newcommand{\hstart}{ \colorlet{shadecolor}{orange!20}
\begin{shaded} }
\newcommand{\hstop}{  \end{shaded} \colorlet{shadecolor}{gray!10}}


# Abstract

The world of brewing is deep and extensive, both in time and number of recipes available. Colorado is home to the most breweries in the United States (with Fort Collins having the most in a Colorado city). As beer enthusiasts ourselves, our team wanted to see if we were able to replicate the perfect beer recipe based off of previous brews. With a little help from data science and use of neural networks, we wanted to create the perfect beer based off of past recipes and their respective ratings. In this paper, we will go over the data and how we were able to extract recipes from the web. We will then go over the inspirations for our paper. Finally, we will explain the methods used to implement our neural net and provide our final results. 

\newpage

# Introduction

Deep learning has left an impact on the world by solving real-world problems which would have otherwise taken decades to discover or complete. There has been a sharp increase in the use of deep learning thanks to more powerful hardware, larger datasets, and an increase in the population of programmers. These effects have allowed humans to create vast advancements in not only the STEM field, but also the creative world. So we decided to explore the world of beer making, with the help of neural networks.

Using deep learning in for generating recipes is not uncommon. There are several tutorials and repositories online that go through this process. Cocktail recipes and cooking recipes have been generated using recurrent neural networks (Bojar, 2019; Trekhleb, 2020). This specific neural net is useful for this task because of how it makes use of sequential information -- perfect for understanding language (Pascanu et al., 2014).

Brewing is a complex problem. There are several methods of brewing, each with different instructions. Every brew involves some method of getting malt into water, boiling to add hops, and fermenting for a number of weeks (Brewing For Beginners, 2019). The methods vary in how the malt is infused with the water, making it wort.

In all grain brewing, malt is added by steeping grains in hot, but not boiling water, then rinsing them in a process called sparging (BrewingTV, 2015). This is the most complex method, but it can offer the most complex flavor profile. Brewing in a bag (BIAB) is a simpler process where grains are added to a bag in the water (How to Brew In A Bag, 2021). Then there is extract only brewing. This is the simplest method, as you add malt extract directly to your boiling water (Brewing For Beginners, 2019). All of these methods are present in our data, making it a varied and complex set.

# Problem Formulation

With millions of different beer recipes in existence, we thought that there was too much "noise" for everyone to try all the recipes. We thought that there were a lot more beers that tasted bad rather than good. So we wanted to see if we can eliminate the noise a little by creating the best recipe for each type of beer (IPA, sour, porter, etc.).

# Data

As there is not a readily available beer recipe database, we had to scrape the ingredients and formulate our own recipes. We wrote a scraper and headed to Brewer's Friend to gather our data. After compiling the data, we found it to be over 100 MB, much too large for our computing power. We deciced to remove any recipes that did not have at least one review on the site, believing that the unreviewed recipes could not be of much repute. We got the size down to around 13 MB, which would contain plenty of observations for the neural net to learn from. The "recipe.csv" dataset contains different beers and their recipes, where each observation (beer recipe) reported method, boil time, hop utilization, and a number of other columns that contain different steps to combine the different ingredients together.

Since it is important to have an understanding of the model's behavior before compilation, we checked into the brewing methods to see if there was an even spread.

```{r}
recipes <- read_csv('Data/recipes.csv')

recipes %>%
  group_by(Method) %>%
  summarize(Count = n()) %>%
  ggplot(aes(Method, Count)) +
  geom_col() +
  ggtitle('Count of Recipes per Brewing Method')
```

Seeing that the recipes are dominated by all grain is not surprising. Most brewers of the caliber to post reputatble recipes will have all grain rigs and be utilizing the freedom that it gives them. This is a problem for the neural network, as it will disproportionately generate all grain recipes, regardless of which style the ingredients call for.

We also wanted to see if brewers were making a specific kind of beer more often. There turned out to be over 100 unique beer styles, but that is because every variation of a style is considered its own style in the data.

```{r}
recipes %>%
  group_by(Style) %>%
  summarize(Count = n()) %>%
  arrange(desc(Count)) %>%
  slice_head(n=5) %>%
  ggplot(aes(reorder(Style, -Count), Count)) +
  geom_col() +
  ggtitle('Top 5 Styles') +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  xlab('Style')
```

A few styles did stand above the rest. A sizeable portion of recipes are American IPAs. In fact, four out of the top five styles are some kind of pale ale. Since this is a craft brewing site, this is unsurprising. Anecdotally, craft brewers are always making their own style of pale ale. This still can have an effect on the results. Recipes might be more likely to have ingredients and follow methods that correspond with brewing pale ales over anything else.

Finally, we must discuss the structure of our recipes. They were hand-written to have a predictable format, using the ingredients data. They are of the following form:

```{python}
print("""NAME
(beer name)

STYLE
(beer style)

METHOD
(brewing method)

INGREDIENTS

  FERMENTABLES
    (list of malts and barleys and the amounts of each)
    
  HOPS
    (list of hops, amounts, usage, and time)
    
  OTHER
    (if there are other ingredients)
    
INSTRUCTIONS
(method-specific brewing instructions)""")
```

This form is replicated in the model output.

# Neural Network/Modeling

The approach for our model came from the methods outlined by Oleksii Trekhleb (2020), Ilya Sutskever (2013), and Daniel Bojar (2019). We used a recurrent neural network with long short-term memory units. This approach is consistent accross similar problems, so we decided to stick with it. After creating the model, and running for approximated four hours and 73 epochs, it was trained. With a user-defined name, style, and brewing method, we are able to generate an original beer recipe, for better or for worse.

```{r}
source_python('./generate_recipe.py')
```

# Conclusion

We have included the inspiration for our study, along with a detailed explanation of how we collected our data and cleaned it to help fit our model better... **explain the outcome of the neural net**

There are a couple problems our group faced in this project that we hope to fix if we were to run it again. We were initially given a couple datasets from a professor at Colorado State University's fermentation department to play around with. However, we found that the dataset didn't have enough observations to feed into the neural network. This is understandable, as Colorado State doesn't exactly have an extensive history of brewing and a large enough facility to test a number of different combinations. In the future, we can also try and communicate with other universities to see if we can utilize their recipes as well (for research purposes). The other problem that we faced is that there is no way to actually verify if the recipe we created actually worked. Although we have our suspicions that our recipes wouldn't work well anyways, it would take months for these recipes to actually be brewed. Regardless of the time constraint at the end of the semester, we might ask the fermentation department for their input on the feasibility of the recipe. 

Overall, we believe that we completed our objective of creating a recipe based off the "best" ingredients and methods from other popular breweries. 

\newpage

# Bibliography
**Must include bibliography**

1. Homebrew Beer Recipes: Browse 200,000+ at Brewer's Friend. Brewer's Friend Beer Brewing Software. (2021). https://www.brewersfriend.com/homebrew-recipes/. 

2. BrewingTV, (2015, August 20), All-Grain Brewing 101: The Basics
[Video]. YouTube. https://www.youtube.com/watch?v=AcRXjdlcvKY

3. How to Brew in a Bag (BIAB). (2021) Homebrew Supply. https://homebrewsupply.com/learn/how-to-brew-in-a-bag-biab/. 

4. Brewing For Beginners - Brewer's Friend. Brewer's Friend | Brewer's Friend is a complete homebrew beer recipe designer with brewing calculators, brew day planner, journal &amp; more. Helping you brew your best, every time! (2019, September 18). https://www.brewersfriend.com/brewing-for-beginners/. 

5. Britz, D. (2016, July 8). Recurrent Neural Networks Tutorial, Part 1 â€“ Introduction to RNNs. WildML. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/. 

6. Bojar, D. (2019, January 5). Generating Novel Cocktail Recipes with a Specific Style through Recurrent Neural Networks. Medium. https://towardsdatascience.com/generating-novel-cocktail-recipes-with-a-specific-style-through-recurrent-neural-networks-4339e9168404. 

7. Trekhleb, O. (2020, July). Generating cooking recipes using TensorFlow and LSTM Recurrent Neural Network: A step-by-step guide. KDnuggets. https://www.kdnuggets.com/2020/07/generating-cooking-recipes-using-tensorflow.html. 

8. Pascanu, R., Gulcehre, C., Cho, K., Bengio, Y. (2014) How to Construct Deep Recurrent Neural Networks. Department of Information and Computer Sience, Aalto University School of Medicine.

9. Sutskever, I. (2013) Training Recurrent Neural Networks. Graduate Department of Computer Science, University of Toronto.